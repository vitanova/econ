{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "search for the job or search for the knowledge\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from textwrap import dedent\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.integrate import fixed_quad as integrate\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats as stats\n",
    "from scipy import interp\n",
    "\n",
    "#the SLSQP method is faster and more stable, but it didn't give the\n",
    "#correct answer in python 3. so, if we are in python 2, use SLSQP, otherwise\n",
    "#use the only other option (to handel constraints): COBYLA\n",
    "if sys.version_info[0] == 2:\n",
    "    method = \"SLSQP\"\n",
    "else:\n",
    "    #python 3\n",
    "    method = \"COBYLA\"\n",
    "    \n",
    "epsilon = 1e-4 # a small, number, used in the optimization routine\n",
    "\n",
    "class JvWorker(object):\n",
    "    r\"\"\"\n",
    "    a Jovanovic-type model of employment with on-the-job search. the\n",
    "    value function is given by\n",
    "    \n",
    "    .. math::\n",
    "    \n",
    "        V(x) = \\max_{\\phi, s} w(x, \\phi, s)\n",
    "        \n",
    "    for \n",
    "    \n",
    "    .. math::\n",
    "    \n",
    "        w(x, \\phi, s) := x(1- \\phi -s)\n",
    "                            + \\beta (1- \\pi(s)) V(G(x, \\phi))\n",
    "                            + \\beta \\pi(s) E V[ \\max(G(x, \\phi), U)]\n",
    "    here\n",
    "    \n",
    "    * x = human capital\n",
    "    * s = search effort\n",
    "    * :math:'\\phi' = investment in human capital\n",
    "    * :math: '\\pi(s)' = probability of new offer given search level s\n",
    "    * :math: 'x(1 - \\phi - s)' = wage\n",
    "    * :math: 'G(x, \\phi)' = new human capital when current job retained\n",
    "    * U = RV with distribution F -- new draw of human capital\n",
    "    \n",
    "    parameters\n",
    "    ------\n",
    "    A: scalar(float), optional(default=1.4)\n",
    "        parameter in human capital transition function\n",
    "    alpha: scalar(float), optional(default=0.6)\n",
    "        parameter in human capital transition function\n",
    "    beta: scalar(float), optional(default=0.96)\n",
    "        discount factor\n",
    "    grid_size: scalar(float), optional(default=50)\n",
    "        grid size for discretization\n",
    "    G: function, optional(default=lambda x, phi: A * (x * phi)**alpha)\n",
    "        transition function for human capital\n",
    "    pi: function, optional(default=sqrt)\n",
    "        function mapping search effort (:math: 's \\in (0, 1)') to\n",
    "        probability of getting new job offer\n",
    "    F: distribution, optional(default=Beta(2, 2))\n",
    "        distribution from which the value of new job offers is drawn\n",
    "        \n",
    "    attributes\n",
    "    ------\n",
    "    A, alpha, beta: see parameters\n",
    "    x_grid: array_like(float)\n",
    "        the grid over the human capital\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, A=1.4, alpha=0.6, beta=0.96, grid_size=50, \n",
    "                G=None, pi=np.sqrt, F=stats.beta(2, 2)):\n",
    "        self.A, self.alpha, self.beta = A, alpha, beta\n",
    "        \n",
    "        #== set defaults for G, pi, and F==\n",
    "        self.G = G if G is not None else lambda x, phi: A * (x * phi)**alpha\n",
    "        self.pi = pi\n",
    "        self.F = F\n",
    "        \n",
    "        #==set up grid over the state space for DP==#\n",
    "        #max of grid is the max of a large quantile value for F and the\n",
    "        #fixed point y = G(y, 1).\n",
    "        grid_max = max(A**(1 / (1 - alpha)), self.F.ppf(1 - epsilon))\n",
    "        self.x_grid = np.linspace(epsilon, grid_max, grid_size)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        m = \"JvWorker(A={a:g}, alpha={al:g}, beta={b:g}, grid_size={gs})\"\n",
    "        return m.format(a=self.A, al=self.alpha, b=self.beta,\n",
    "                       gs=self.x_grid.size)\n",
    "    \n",
    "    def __str__(self):\n",
    "        m = \"\"\"\\\n",
    "        Jovanovic worker (on the job search):\n",
    "            - A (parameter in human capital transition function): {a:g}\n",
    "            - alpha (parameter in human capital transition function): {al: g}\n",
    "            - beta (parameter in human capital transition function): {b:g}\n",
    "            - grid_size (number of grid points for human capital): {gs}\n",
    "            - grid_max (maximum of grid for human capital): {gm:g}\n",
    "        \"\"\"\n",
    "        return dedent(m.format(a=self.A, al=self.alpha, b=self.beta,\n",
    "                              gs=self.x_grid.size, gm=self.x_grid.max()))\n",
    "    \n",
    "    def bellman_operator(self, V, brute_force=False, return_policies=False):\n",
    "        \"\"\"\n",
    "        returns the approximate value function TV by applying the\n",
    "        Bellman operator associated with the model to the function V.\n",
    "        \n",
    "        returns TV, or the V-greedy policies s_policy and phi_policy when\n",
    "        return_policies=True. In the function, the array V is replaced below\n",
    "        with a function Vf that implements linear interpolation over the \n",
    "        points (V(x), x) for x in x_grid.\n",
    "        \n",
    "        parameters\n",
    "        ------\n",
    "        V: array_like(float)\n",
    "            array representing an approximate value function\n",
    "        brute_force: bool, optional(default=False)\n",
    "            default is False. if the brute_force flag is True, then grid\n",
    "            search is performed at each maximization step.\n",
    "        return_policies: bool, optional(default=False)\n",
    "            indicates whether to return just the updated value function\n",
    "            TV or both the greedy policy computed from V and TV\n",
    "            \n",
    "        returns\n",
    "        ------\n",
    "        s_policy: array_like(float)\n",
    "            the greedy policy computed from V. only returned if\n",
    "            return_policies == True\n",
    "        new_V: array_like(float)\n",
    "            the updated value function Tv, as an array representing the\n",
    "            values TV(x) over x in x_grid.\n",
    "            \n",
    "        \"\"\"\n",
    "        #==simplify names, set up arrays, etc.==#\n",
    "        G, pi, F, beta = self.G, self.pi, self.F, self.beta\n",
    "        Vf = lambda x: interp(x, self.x_grid, V)\n",
    "        N = len(self.x_grid)\n",
    "        new_V, s_policy, phi_policy = np.empty(N), np.empty(N), np.empty(N)\n",
    "        a, b = F.ppf(0.005), F.ppf(0.995) #quantiles, for integration\n",
    "        c1 = lambda z: 1.0 - sum(z) #used to enforce s + phi <=1\n",
    "        c2 = lambda z: z[0] - epsilon #used to enforce s>= epsilon\n",
    "        c3 = lambda z: z[1] - epsilon # used to enforce phi >= epsilon\n",
    "        guess = (0.2, 0.2)\n",
    "        constraints = [{\"type\": \"ineq\", \"fun\": i} for i in [c1, c2, c3]]\n",
    "        \n",
    "        #==solve r.h.s of Bellman equation==#\n",
    "        for i, x in enumerate(self.x_grid):\n",
    "            \n",
    "            #set up objective function==#\n",
    "            def w(z):\n",
    "                s, phi = z\n",
    "                h = lambda u: Vf(np.maximum(G(x, phi), u)) * F.pdf(u)\n",
    "                integral, err = integrate(h, a, b)\n",
    "                q = pi(s) * integral + (1.0 - pi(s)) * Vf(G(x, phi))\n",
    "                #==minus because we minimize==#\n",
    "                return - x * (1.0 - phi - s) - beta * q\n",
    "            \n",
    "            #==either use scipy solver==#\n",
    "            if not brute_force:\n",
    "                max_s, max_phi = minimize(w, guess, constraints =constraints, \n",
    "                                         options={\"disp\": 0},\n",
    "                                         method=method)[\"x\"]\n",
    "                max_val = -w((max_s, max_phi))\n",
    "                \n",
    "            #==or search on a grid==#\n",
    "            else:\n",
    "                search_grid = np.linspace(epsilon, 1.0, 15)\n",
    "                max_val = -1.0\n",
    "                for s in search_grid:\n",
    "                    for phi in search_grid:\n",
    "                        current_val = -w((s, phi)) if s + phi <=1.0 else -1.0\n",
    "                        if current_val > max_val:\n",
    "                            max_val, max_s, max_phi = current_val, s, phi\n",
    "                            \n",
    "            #==store results==#\n",
    "            new_V[i] = max_val\n",
    "            s_policy[i], phi_policy[i] = max_s, max_phi\n",
    "            \n",
    "        if return_policies:\n",
    "            return s_policy, phi_policy\n",
    "        else:\n",
    "            return new_V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
